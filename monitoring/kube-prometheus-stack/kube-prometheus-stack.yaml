---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
  namespace: monitoring
spec:
  interval: 5m
  chart:
    spec:
      # renovate: registryUrl=https://prometheus-community.github.io/helm-charts
      chart: kube-prometheus-stack
      version: 12.12.2
      sourceRef:
        kind: HelmRepository
        name: prometheus-community-charts
        namespace: flux-system
      interval: 5m
  timeout: 20m
  values:
    server:
      resources:
        requests:
          memory: 1500Mi
          cpu: 25m
        limits:
          memory: 2000Mi
    prometheusOperator:
      createCustomResource: true
      configReloaderCpu: 200m
    alertmanager:
      alertmanagerSpec:
        storage:
          volumeClaimTemplate:
            spec:
              storageClassName: local-path
              resources:
                requests:
                  storage: 10Gi
      ingress:
        enabled: true
        annotations:
          kubernetes.io/ingress.class: "nginx"
          nginx.ingress.kubernetes.io/auth-url: "https://auth.architectsmp.com/oauth2/auth"
          nginx.ingress.kubernetes.io/auth-signin: https://auth.architectsmp.com/oauth2/start
        hosts: [prom-alert.architectsmp.com]
        tls:
          - hosts:
              - prom-alert.architectsmp.com
      config:
        global:
          resolve_timeout: 5m
        route:
          receiver: "slack-notifications"
          routes:
            - match:
                alertname: Watchdog
              receiver: "null"
            - receiver: "slack-notifications"
              match:
                severity: critical
              continue: true
            - receiver: "slack-notifications"
        inhibit_rules:
          - source_match:
              severity: "critical"
            target_match:
              severity: "warning"
            # Apply inhibition if the alertname is the same.
            equal: ["alertname", "namespace"]
        templates: ["*.tmpl"]
    nodeExporter:
      serviceMonitor:
        relabelings:
          - action: replace
            regex: (.*)
            replacement: $1
            sourceLabels:
              - __meta_kubernetes_pod_node_name
            targetLabel: kubernetes_node
    kubelet:
      serviceMonitor:
        metricRelabelings:
          - action: replace
            sourceLabels:
              - node
            targetLabel: instance
    grafana:
      deploymentStrategy:
        type: Recreate
      podAnnotations:
        backup.velero.io/backup-volumes: storage
      persistence:
        enabled: true
        storageClassName: "local-path"
        size: 10Gi
        accessModes:
          - ReadWriteOnce
      env:
        GF_EXPLORE_ENABLED: true
        GF_DISABLE_SANITIZE_HTML: true
        GF_PANELS_DISABLE_SANITIZE_HTML: true
      ingress:
        enabled: true
        annotations:
          kubernetes.io/ingress.class: "nginx"
        hosts: [grafana.architectsmp.com]
        tls:
          - hosts:
              - grafana.architectsmp.com
      plugins:
        - natel-discrete-panel
        - pr0ps-trackmap-panel
        - grafana-piechart-panel
        - grafana-clock-panel
        - https://github.com/panodata/grafana-map-panel/releases/download/0.9.0/grafana-map-panel-0.9.0.zip;grafana-worldmap-panel-ng
      dashboardProviders:
        dashboardproviders.yaml:
          apiVersion: 1
          providers:
            - name: "default"
              orgId: 1
              folder: ""
              type: file
              disableDeletion: false
              editable: true
              options:
                path: /var/lib/grafana/dashboards/default
      dashboards:
        default:
          flux-cluster:
            url: https://raw.githubusercontent.com/fluxcd/flux2/v0.4.2/manifests/monitoring/grafana/dashboards/cluster.json
            datasource: Prometheus
          flux-control-plane:
            url: https://raw.githubusercontent.com/fluxcd/flux2/v0.4.2/manifests/monitoring/grafana/dashboards/control-plane.json
            datasource: Prometheus
          nginx-dashboard:
            url: https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/grafana/dashboards/nginx.json
            datasource: Prometheus
          1-node-exporter:
            url: https://grafana.com/api/dashboards/11074/revisions/9/download
            datasource: Prometheus
      sidecar:
        datasources:
          enabled: true
          defaultDatasourceEnabled: false
        dashboards:
          enabled: true
          searchNamespace: ALL
      additionalDataSources:
        - name: Prometheus
          type: prometheus
          access: proxy
          url: http://thanos-query-http:10902/
          isDefault: true
        - name: loki
          type: loki
          access: proxy
          url: http://loki.logs.svc.cluster.local:3100
      grafana.ini:
        paths:
          data: /var/lib/grafana/data
          logs: /var/log/grafana
          plugins: /var/lib/grafana/plugins
          provisioning: /etc/grafana/provisioning
        analytics:
          check_for_updates: true
        log:
          mode: console
        grafana_net:
          url: https://grafana.net
    kubeEtcd:
      enabled: false
    kubeControllerManager:
      enabled: false
    kubeScheduler:
      enabled: false
    kubeProxy:
      enabled: false
    prometheus-node-exporter:
      tolerations:
        - key: "node-role.kubernetes.io/master"
          operator: "Exists"
    prometheus:
      ingress:
        enabled: true
        annotations:
          kubernetes.io/ingress.class: "nginx"
          nginx.ingress.kubernetes.io/auth-url: "https://auth.architectsmp.com/oauth2/auth"
          nginx.ingress.kubernetes.io/auth-signin: https://auth.architectsmp.com/oauth2/start
        hosts: [prom-server.architectsmp.com]
        tls:
          - hosts:
              - prom-server.architectsmp.com
      prometheusSpec:
        replicas: 2
        replicaExternalLabelName: "replica"
        ruleSelector: {}
        ruleNamespaceSelector: {}
        ruleSelectorNilUsesHelmValues: false
        serviceMonitorSelector: {}
        serviceMonitorNamespaceSelector: {}
        serviceMonitorSelectorNilUsesHelmValues: false
        podMonitorSelector: {}
        podMonitorNamespaceSelector: {}
        podMonitorSelectorNilUsesHelmValues: false
        retention: 6h
        enableAdminAPI: true
        walCompression: true
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: local-path
              resources:
                requests:
                  storage: 10Gi
        thanos:
          image: quay.io/thanos/thanos:v0.15.0
          version: v0.15.0
          objectStorageConfig:
            name: thanos
            key: object-store.yaml
  valuesFrom:
    - kind: Secret
      name: "kube-prometheus-stack-helm-values"
      optional: false
